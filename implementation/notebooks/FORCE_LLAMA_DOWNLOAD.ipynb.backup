{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü¶ô FORCE LLaMA Download - Real LLaMA Only!\n",
    "\n",
    "This notebook **forces** the download and use of actual LLaMA models, ignoring system limitations.\n",
    "\n",
    "**You want LLaMA only - this will get you LLaMA only!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install llama-cpp-python for Windows (force install)\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"üîß Force installing llama-cpp-python for Windows...\")\n",
    "print(\"This may take 5-10 minutes...\")\n",
    "\n",
    "# Try multiple installation methods\n",
    "methods = [\n",
    "    # Method 1: Precompiled wheel\n",
    "    \"pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cpu\",\n",
    "    # Method 2: Force reinstall\n",
    "    \"pip install llama-cpp-python --force-reinstall --no-cache-dir\",\n",
    "    # Method 3: Latest version\n",
    "    \"pip install llama-cpp-python>=0.2.11\"\n",
    "]\n",
    "\n",
    "for i, method in enumerate(methods, 1):\n",
    "    print(f\"\\nüîß Trying installation method {i}: {method}\")\n",
    "    try:\n",
    "        result = subprocess.run(method, shell=True, check=True, capture_output=True, text=True)\n",
    "        print(f\"‚úÖ Method {i} succeeded!\")\n",
    "        break\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ùå Method {i} failed: {e.stderr[:200]}...\")\n",
    "        continue\n",
    "\n",
    "# Test if it worked\n",
    "try:\n",
    "    from llama_cpp import Llama\n",
    "    print(\"\\nüéâ llama-cpp-python successfully installed!\")\n",
    "    llama_cpp_available = True\n",
    "except ImportError:\n",
    "    print(\"\\n‚ö†Ô∏è  llama-cpp-python still not available\")\n",
    "    print(\"Will use HuggingFace transformers with actual LLaMA models\")\n",
    "    llama_cpp_available = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force download LLaMA 7B model (ignoring system limitations)\n",
    "import os\n",
    "from pathlib import Path\n",
    "from huggingface_hub import hf_hub_download\n",
    "import time\n",
    "\n",
    "print(\"ü¶ô FORCE DOWNLOADING LLaMA 7B MODEL\")\n",
    "print(\"Ignoring system RAM limitations - you want LLaMA, you get LLaMA!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# LLaMA 7B configuration\n",
    "model_config = {\n",
    "    \"repo_id\": \"TheBloke/Llama-2-7B-Chat-GGUF\",\n",
    "    \"filename\": \"llama-2-7b-chat.Q4_K_M.gguf\",\n",
    "    \"size_gb\": 3.8\n",
    "}\n",
    "\n",
    "# Create models directory\n",
    "model_dir = Path(\"../models/llama\")\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_path = model_dir / model_config[\"filename\"]\n",
    "\n",
    "# Check if already downloaded\n",
    "if model_path.exists():\n",
    "    print(f\"‚úÖ LLaMA model already exists: {model_path}\")\n",
    "    print(f\"üìä File size: {model_path.stat().st_size / (1024**3):.1f} GB\")\n",
    "else:\n",
    "    print(f\"‚¨áÔ∏è  Downloading LLaMA 7B model...\")\n",
    "    print(f\"   Repository: {model_config['repo_id']}\")\n",
    "    print(f\"   File: {model_config['filename']}\")\n",
    "    print(f\"   Size: ~{model_config['size_gb']} GB\")\n",
    "    print(f\"   This will take 10-30 minutes depending on internet speed...\")\n",
    "    print()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        downloaded_path = hf_hub_download(\n",
    "            repo_id=model_config[\"repo_id\"],\n",
    "            filename=model_config[\"filename\"],\n",
    "            local_dir=str(model_dir),\n",
    "            local_dir_use_symlinks=False\n",
    "        )\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"\\n‚úÖ LLaMA download completed in {elapsed/60:.1f} minutes!\")\n",
    "        print(f\"   Model saved to: {downloaded_path}\")\n",
    "        model_path = Path(downloaded_path)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Download failed: {e}\")\n",
    "        raise\n",
    "\n",
    "print(f\"\\nüéØ LLaMA model ready at: {model_path}\")\n",
    "llama_model_path = str(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test LLaMA model with llama-cpp-python\n",
    "print(\"üß™ Testing LLaMA model...\")\n",
    "\n",
    "if llama_cpp_available:\n",
    "    try:\n",
    "        from llama_cpp import Llama\n",
    "        \n",
    "        print(\"‚ö° Loading LLaMA model with llama-cpp-python...\")\n",
    "        print(\"(This may take 1-2 minutes)\")\n",
    "        \n",
    "        # Initialize LLaMA model\n",
    "        llama = Llama(\n",
    "            model_path=llama_model_path,\n",
    "            n_ctx=2048,\n",
    "            n_batch=512,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ LLaMA model loaded successfully!\")\n",
    "        \n",
    "        # Test with customer support prompt\n",
    "        test_prompt = \"\"\"<s>[INST] <<SYS>>\n",
    "You are an AI assistant specialized in customer support ticket analysis. \n",
    "Analyze the support ticket and provide a JSON response with the following fields:\n",
    "- category: one of [billing, technical, general_inquiry, complaint, compliment, account]\n",
    "- priority: one of [high, medium, low] \n",
    "- estimated_hours: float between 0.5 and 48.0\n",
    "- tags: list of 2-4 relevant keywords\n",
    "- sentiment: one of [positive, negative, neutral]\n",
    "<</SYS>>\n",
    "\n",
    "Ticket: My billing statement shows duplicate charges for this month\n",
    "\n",
    "Analyze this ticket and respond with JSON: [/INST]\"\"\"\n",
    "        \n",
    "        print(\"\\nüîç Testing LLaMA with customer support ticket...\")\n",
    "        \n",
    "        response = llama(\n",
    "            test_prompt,\n",
    "            max_tokens=256,\n",
    "            temperature=0.1,\n",
    "            top_p=0.95,\n",
    "            top_k=50,\n",
    "            repeat_penalty=1.1,\n",
    "            echo=False\n",
    "        )\n",
    "        \n",
    "        generated_text = response['choices'][0]['text'].strip()\n",
    "        \n",
    "        print(\"\\nüéâ LLaMA Response:\")\n",
    "        print(\"=\" * 40)\n",
    "        print(generated_text)\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # Clean up memory\n",
    "        del llama\n",
    "        \n",
    "        print(\"\\n‚úÖ LLaMA test successful! Real LLaMA is working!\")\n",
    "        llama_working = True\n",
    "        test_method = \"llama-cpp\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå LLaMA llama-cpp test failed: {e}\")\n",
    "        llama_working = False\n",
    "        test_method = \"failed\"\n",
    "else:\n",
    "    llama_working = False\n",
    "    test_method = \"not_available\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Test with HuggingFace Transformers (still real LLaMA)\n",
    "if not llama_working:\n",
    "    print(\"üîÑ Testing LLaMA with HuggingFace Transformers...\")\n",
    "    print(\"This is still REAL LLaMA - just loaded differently!\")\n",
    "    \n",
    "    try:\n",
    "        from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "        import torch\n",
    "        \n",
    "        model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "        \n",
    "        print(f\"‚ö° Loading LLaMA from HuggingFace: {model_name}\")\n",
    "        print(\"Note: This requires HuggingFace access token for LLaMA models\")\n",
    "        print(\"If this fails, that's okay - we'll use the downloaded GGUF model\")\n",
    "        \n",
    "        # Try to load tokenizer and model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        \n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        print(\"‚úÖ LLaMA loaded with HuggingFace Transformers!\")\n",
    "        \n",
    "        # Test prompt\n",
    "        test_input = \"Classify this customer support ticket: My billing shows duplicate charges. Respond with category, priority, and sentiment.\"\n",
    "        \n",
    "        inputs = tokenizer.encode(test_input, return_tensors='pt', truncation=True, max_length=512)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs,\n",
    "                max_new_tokens=100,\n",
    "                temperature=0.1,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        generated_part = response[len(test_input):].strip()\n",
    "        \n",
    "        print(\"\\nüéâ LLaMA Transformers Response:\")\n",
    "        print(\"=\" * 40)\n",
    "        print(generated_part)\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        print(\"\\n‚úÖ LLaMA with Transformers successful!\")\n",
    "        llama_working = True\n",
    "        test_method = \"transformers\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå HuggingFace LLaMA failed: {e}\")\n",
    "        print(\"This might be due to missing HuggingFace access token\")\n",
    "        print(\"But that's okay - we have the GGUF model downloaded!\")\n",
    "        llama_working = \"partial\"\n",
    "        test_method = \"gguf_only\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save forced LLaMA configuration\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "config_dir = Path(\"../outputs\")\n",
    "config_dir.mkdir(exist_ok=True)\n",
    "\n",
    "forced_config = {\n",
    "    \"forced_llama\": True,\n",
    "    \"llama_model_path\": llama_model_path,\n",
    "    \"llama_working\": llama_working,\n",
    "    \"test_method\": test_method,\n",
    "    \"llama_cpp_available\": llama_cpp_available,\n",
    "    \"model_size\": \"7B\",\n",
    "    \"download_forced\": True,\n",
    "    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "}\n",
    "\n",
    "with open(config_dir / \"forced_llama_config.json\", \"w\") as f:\n",
    "    json.dump(forced_config, f, indent=2)\n",
    "\n",
    "print(\"üíæ Forced LLaMA configuration saved!\")\n",
    "print(f\"   Config: {config_dir}/forced_llama_config.json\")\n",
    "\n",
    "if llama_working:\n",
    "    print(\"\\nüéâ SUCCESS: You now have REAL LLaMA working!\")\n",
    "    print(\"‚úÖ LLaMA 7B model downloaded and tested\")\n",
    "    print(f\"‚úÖ Working method: {test_method}\")\n",
    "    print(\"‚úÖ Ready for customer support ticket processing\")\n",
    "    print(\"\\nüöÄ Next steps:\")\n",
    "    print(\"1. Run 02_model_setup_and_configuration.ipynb\")\n",
    "    print(\"2. The system will now use your LLaMA model!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  LLaMA testing had issues, but model is downloaded\")\n",
    "    print(\"‚úÖ LLaMA GGUF model is ready at:\", llama_model_path)\n",
    "    print(\"Try running 02_model_setup_and_configuration.ipynb\")\n",
    "    print(\"The system will attempt to use LLaMA there\")\n",
    "\n",
    "print(\"\\nü¶ô FORCE LLaMA Setup Complete!\")"
   ]
  }
 ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.10.0\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 4\n}