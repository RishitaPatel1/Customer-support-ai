{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Optimized ETA Prediction System\n",
    "\n",
    "**OPTIMIZED VERSION**\n",
    "\n",
    "Advanced ETA prediction system with:\n",
    "- Machine learning models trained on full dataset\n",
    "- Real-time complexity analysis\n",
    "- Historical data pattern learning\n",
    "- Advanced workload optimization\n",
    "- Dynamic prediction refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import warnings\n",
    "import gc\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=== OPTIMIZED ETA Prediction System ===\")\n",
    "print(\"Advanced ML-powered time estimation with full dataset training\")\n",
    "print(\"Real-time complexity analysis and workload optimization\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load optimized classification results and full dataset\n",
    "def load_classification_and_training_data():\n",
    "    \"\"\"Load both classification results and full training data\"\"\"\n",
    "    \n",
    "    # Load optimized classification results\n",
    "    classification_path = Path(\"../outputs/optimized_ticket_classifications.csv\")\n",
    "    if classification_path.exists():\n",
    "        classification_df = pd.read_csv(classification_path)\n",
    "        print(f\"‚úÖ Loaded {len(classification_df)} classified tickets\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Optimized classification results not found. Please run optimized notebook 03 first.\")\n",
    "    \n",
    "    # Load full training dataset for ML model training\n",
    "    training_datasets = []\n",
    "    data_files = [\n",
    "        '../data/processed/train_data.csv',\n",
    "        '../data/processed/val_data.csv',\n",
    "        '../data/processed/test_data.csv',\n",
    "        '../data/processed/full_dataset.csv'\n",
    "    ]\n",
    "    \n",
    "    for file_path in data_files:\n",
    "        if Path(file_path).exists():\n",
    "            df = pd.read_csv(file_path)\n",
    "            training_datasets.append(df)\n",
    "            print(f\"‚úÖ Loaded {len(df)} samples from {Path(file_path).name}\")\n",
    "    \n",
    "    if training_datasets:\n",
    "        full_training_data = pd.concat(training_datasets, ignore_index=True)\n",
    "        full_training_data = full_training_data.drop_duplicates(subset=['text'], keep='first')\n",
    "        print(f\"üìä Total training data: {len(full_training_data):,} unique samples\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"No training data found. Please run notebook 01 first.\")\n",
    "    \n",
    "    # Load model configuration\n",
    "    config_path = Path(\"../outputs/optimized_model_config.json\")\n",
    "    if config_path.exists():\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = json.load(f)\n",
    "        print(\"‚úÖ Optimized model configuration loaded\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Optimized configuration not found. Please run optimized notebook 02 first.\")\n",
    "    \n",
    "    return classification_df, full_training_data, config\n",
    "\n",
    "# Load all data\n",
    "classification_results, full_training_data, model_config = load_classification_and_training_data()\n",
    "\n",
    "print(f\"\\nüìà Data Summary:\")\n",
    "print(f\"- Classification results: {len(classification_results)} tickets\")\n",
    "print(f\"- Full training data: {len(full_training_data):,} tickets\")\n",
    "print(f\"- ETA range in training data: {full_training_data['estimated_hours'].min():.1f} - {full_training_data['estimated_hours'].max():.1f} hours\")\n",
    "print(f\"- Average ETA in training: {full_training_data['estimated_hours'].mean():.1f} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced ETA Prediction System with ML training\n",
    "class AdvancedETAPredictor:\n",
    "    \"\"\"Advanced ETA predictor with machine learning and real data training\"\"\"\n",
    "    \n",
    "    def __init__(self, training_data, config):\n",
    "        self.training_data = training_data\n",
    "        self.config = config\n",
    "        self.categories = config['categories']\n",
    "        self.priority_levels = config['priority_levels']\n",
    "        \n",
    "        # ML models\n",
    "        self.rf_model = None\n",
    "        self.gb_model = None\n",
    "        self.tfidf_vectorizer = None\n",
    "        self.label_encoders = {}\n",
    "        self.scaler = None\n",
    "        \n",
    "        # Training performance metrics\n",
    "        self.model_performance = {}\n",
    "        \n",
    "        # Advanced complexity patterns learned from data\n",
    "        self.complexity_patterns = self._analyze_complexity_patterns()\n",
    "        \n",
    "        # Historical ETA patterns\n",
    "        self.historical_patterns = self._analyze_historical_patterns()\n",
    "        \n",
    "    def _analyze_complexity_patterns(self):\n",
    "        \"\"\"Learn complexity patterns from real training data\"\"\"\n",
    "        print(\"üß† Learning complexity patterns from training data...\")\n",
    "        \n",
    "        patterns = {}\n",
    "        \n",
    "        # Analyze text length vs ETA\n",
    "        self.training_data['text_length'] = self.training_data['text'].str.len()\n",
    "        self.training_data['word_count'] = self.training_data['text'].str.split().str.len()\n",
    "        \n",
    "        # Text complexity indicators\n",
    "        complexity_words = {\n",
    "            'simple': ['help', 'how', 'what', 'when', 'where', 'question'],\n",
    "            'moderate': ['problem', 'issue', 'error', 'trouble', 'support'],\n",
    "            'complex': ['multiple', 'several', 'complicated', 'integration', 'system', 'database', 'server', 'network']\n",
    "        }\n",
    "        \n",
    "        for complexity, words in complexity_words.items():\n",
    "            # Find avg ETA for tickets with these complexity indicators\n",
    "            mask = self.training_data['text'].str.lower().str.contains('|'.join(words), case=False, na=False)\n",
    "            if mask.any():\n",
    "                avg_eta = self.training_data[mask]['estimated_hours'].mean()\n",
    "                count = mask.sum()\n",
    "                patterns[complexity] = {\n",
    "                    'avg_eta': avg_eta,\n",
    "                    'sample_count': count,\n",
    "                    'keywords': words\n",
    "                }\n",
    "                print(f\"  {complexity}: {avg_eta:.1f}h avg ETA ({count} samples)\")\n",
    "        \n",
    "        # Category-specific complexity\n",
    "        patterns['category_complexity'] = {}\n",
    "        for category in self.categories:\n",
    "            cat_data = self.training_data[self.training_data['category'] == category]\n",
    "            if len(cat_data) > 0:\n",
    "                patterns['category_complexity'][category] = {\n",
    "                    'avg_eta': cat_data['estimated_hours'].mean(),\n",
    "                    'std_eta': cat_data['estimated_hours'].std(),\n",
    "                    'min_eta': cat_data['estimated_hours'].min(),\n",
    "                    'max_eta': cat_data['estimated_hours'].max(),\n",
    "                    'sample_count': len(cat_data)\n",
    "                }\n",
    "        \n",
    "        return patterns\n",
    "    \n",
    "    def _analyze_historical_patterns(self):\n",
    "        \"\"\"Analyze historical patterns for better prediction\"\"\"\n",
    "        print(\"üìä Analyzing historical ETA patterns...\")\n",
    "        \n",
    "        patterns = {}\n",
    "        \n",
    "        # Priority vs ETA correlation\n",
    "        patterns['priority_eta'] = {}\n",
    "        for priority in self.priority_levels:\n",
    "            priority_data = self.training_data[self.training_data['priority'] == priority]\n",
    "            if len(priority_data) > 0:\n",
    "                patterns['priority_eta'][priority] = {\n",
    "                    'avg': priority_data['estimated_hours'].mean(),\n",
    "                    'median': priority_data['estimated_hours'].median(),\n",
    "                    'std': priority_data['estimated_hours'].std()\n",
    "                }\n",
    "        \n",
    "        # Text length correlation\n",
    "        text_lengths = self.training_data['text_length']\n",
    "        eta_values = self.training_data['estimated_hours']\n",
    "        correlation = np.corrcoef(text_lengths, eta_values)[0, 1]\n",
    "        patterns['text_length_correlation'] = correlation\n",
    "        \n",
    "        print(f\"  Text length-ETA correlation: {correlation:.3f}\")\n",
    "        \n",
    "        return patterns\n",
    "    \n",
    "    def prepare_features(self, data):\n",
    "        \"\"\"Prepare advanced features for ML training\"\"\"\n",
    "        features_df = data.copy()\n",
    "        \n",
    "        # Text features\n",
    "        features_df['text_length'] = features_df['text'].str.len()\n",
    "        features_df['word_count'] = features_df['text'].str.split().str.len()\n",
    "        features_df['avg_word_length'] = features_df['text'].apply(lambda x: np.mean([len(word) for word in str(x).split()]) if str(x).split() else 0)\n",
    "        \n",
    "        # Complexity indicators\n",
    "        features_df['has_urgency'] = features_df['text'].str.lower().str.contains('urgent|emergency|asap|critical', case=False, na=False)\n",
    "        features_df['has_technical_terms'] = features_df['text'].str.lower().str.contains('error|bug|crash|system|server|database', case=False, na=False)\n",
    "        features_df['question_marks'] = features_df['text'].str.count('\\?')\n",
    "        features_df['exclamation_marks'] = features_df['text'].str.count('!')\n",
    "        \n",
    "        # Categorical features encoding\n",
    "        for col in ['category', 'priority']:\n",
    "            if col in features_df.columns:\n",
    "                if col not in self.label_encoders:\n",
    "                    self.label_encoders[col] = LabelEncoder()\n",
    "                    features_df[f'{col}_encoded'] = self.label_encoders[col].fit_transform(features_df[col].astype(str))\n",
    "                else:\n",
    "                    # Handle unseen categories\n",
    "                    known_classes = set(self.label_encoders[col].classes_)\n",
    "                    features_df[col] = features_df[col].apply(lambda x: x if x in known_classes else 'unknown')\n",
    "                    \n",
    "                    # Add 'unknown' to classes if not present\n",
    "                    if 'unknown' not in known_classes:\n",
    "                        self.label_encoders[col].classes_ = np.append(self.label_encoders[col].classes_, 'unknown')\n",
    "                    \n",
    "                    features_df[f'{col}_encoded'] = self.label_encoders[col].transform(features_df[col].astype(str))\n",
    "        \n",
    "        # Select numeric features for ML\n",
    "        feature_columns = [\n",
    "            'text_length', 'word_count', 'avg_word_length',\n",
    "            'question_marks', 'exclamation_marks',\n",
    "            'category_encoded', 'priority_encoded'\n",
    "        ]\n",
    "        \n",
    "        # Add boolean features as int\n",
    "        bool_columns = ['has_urgency', 'has_technical_terms']\n",
    "        for col in bool_columns:\n",
    "            features_df[col] = features_df[col].astype(int)\n",
    "            feature_columns.append(col)\n",
    "        \n",
    "        return features_df[feature_columns]\n",
    "    \n",
    "    def train_ml_models(self):\n",
    "        \"\"\"Train ML models on full dataset\"\"\"\n",
    "        print(f\"\\nü§ñ Training ML models on {len(self.training_data):,} samples...\")\n",
    "        \n",
    "        # Prepare features and target\n",
    "        X = self.prepare_features(self.training_data)\n",
    "        y = self.training_data['estimated_hours'].values\n",
    "        \n",
    "        # Train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Scale features\n",
    "        self.scaler = StandardScaler()\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "        # Train Random Forest\n",
    "        print(\"Training Random Forest model...\")\n",
    "        self.rf_model = RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=15,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        self.rf_model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Train Gradient Boosting\n",
    "        print(\"Training Gradient Boosting model...\")\n",
    "        self.gb_model = GradientBoostingRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=8,\n",
    "            learning_rate=0.1,\n",
    "            random_state=42\n",
    "        )\n",
    "        self.gb_model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Evaluate models\n",
    "        rf_pred = self.rf_model.predict(X_test_scaled)\n",
    "        gb_pred = self.gb_model.predict(X_test_scaled)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        rf_mae = mean_absolute_error(y_test, rf_pred)\n",
    "        rf_r2 = r2_score(y_test, rf_pred)\n",
    "        \n",
    "        gb_mae = mean_absolute_error(y_test, gb_pred)\n",
    "        gb_r2 = r2_score(y_test, gb_pred)\n",
    "        \n",
    "        self.model_performance = {\n",
    "            'random_forest': {'mae': rf_mae, 'r2': rf_r2},\n",
    "            'gradient_boosting': {'mae': gb_mae, 'r2': gb_r2},\n",
    "            'training_samples': len(X_train),\n",
    "            'test_samples': len(X_test)\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nüìä Model Performance:\")\n",
    "        print(f\"Random Forest - MAE: {rf_mae:.2f}h, R¬≤: {rf_r2:.3f}\")\n",
    "        print(f\"Gradient Boosting - MAE: {gb_mae:.2f}h, R¬≤: {gb_r2:.3f}\")\n",
    "        \n",
    "        # Feature importance\n",
    "        feature_names = X.columns\n",
    "        rf_importance = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': self.rf_model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(f\"\\nüéØ Top Feature Importance (Random Forest):\")\n",
    "        for _, row in rf_importance.head(5).iterrows():\n",
    "            print(f\"  {row['feature']}: {row['importance']:.3f}\")\n",
    "        \n",
    "        return X_test_scaled, y_test, rf_pred, gb_pred\n",
    "    \n",
    "    def predict_eta_advanced(self, ticket_data):\n",
    "        \"\"\"Advanced ETA prediction using ensemble of methods\"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for _, row in ticket_data.iterrows():\n",
    "            # ML prediction\n",
    "            ml_eta = self._predict_with_ml(row)\n",
    "            \n",
    "            # Pattern-based prediction\n",
    "            pattern_eta = self._predict_with_patterns(row)\n",
    "            \n",
    "            # Ensemble prediction (weighted average)\n",
    "            final_eta = 0.7 * ml_eta + 0.3 * pattern_eta\n",
    "            \n",
    "            # Apply constraints\n",
    "            final_eta = max(0.1, min(final_eta, 168.0))  # Between 6 minutes and 1 week\n",
    "            \n",
    "            prediction = {\n",
    "                'ticket_text': row['ticket_text'],\n",
    "                'predicted_category': row.get('predicted_category', row.get('category', 'unknown')),\n",
    "                'predicted_priority': row.get('predicted_priority', row.get('priority', 'medium')),\n",
    "                'predicted_sentiment': row.get('predicted_sentiment', 'neutral'),\n",
    "                'ml_eta': ml_eta,\n",
    "                'pattern_eta': pattern_eta,\n",
    "                'final_eta': round(final_eta, 1),\n",
    "                'complexity': self._analyze_ticket_complexity(row['ticket_text']),\n",
    "                'confidence': self._calculate_confidence(ml_eta, pattern_eta),\n",
    "                'estimated_completion': datetime.now() + timedelta(hours=final_eta)\n",
    "            }\n",
    "            \n",
    "            predictions.append(prediction)\n",
    "        \n",
    "        return pd.DataFrame(predictions)\n",
    "    \n",
    "    def _predict_with_ml(self, row):\n",
    "        \"\"\"ML-based ETA prediction\"\"\"\n",
    "        if self.rf_model is None or self.gb_model is None:\n",
    "            return 2.0  # Default fallback\n",
    "        \n",
    "        # Create single-row DataFrame for feature preparation\n",
    "        temp_df = pd.DataFrame([row])\n",
    "        \n",
    "        # Prepare features\n",
    "        try:\n",
    "            features = self.prepare_features(temp_df)\n",
    "            features_scaled = self.scaler.transform(features)\n",
    "            \n",
    "            # Ensemble prediction\n",
    "            rf_pred = self.rf_model.predict(features_scaled)[0]\n",
    "            gb_pred = self.gb_model.predict(features_scaled)[0]\n",
    "            \n",
    "            # Weighted ensemble\n",
    "            ml_prediction = 0.6 * rf_pred + 0.4 * gb_pred\n",
    "            \n",
    "            return max(0.1, ml_prediction)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ML prediction error: {e}\")\n",
    "            return 2.0\n",
    "    \n",
    "    def _predict_with_patterns(self, row):\n",
    "        \"\"\"Pattern-based ETA prediction using learned patterns\"\"\"\n",
    "        category = row.get('predicted_category', row.get('category', 'general_inquiry'))\n",
    "        priority = row.get('predicted_priority', row.get('priority', 'medium'))\n",
    "        \n",
    "        # Base ETA from category patterns\n",
    "        if category in self.complexity_patterns.get('category_complexity', {}):\n",
    "            base_eta = self.complexity_patterns['category_complexity'][category]['avg_eta']\n",
    "        else:\n",
    "            base_eta = 2.0\n",
    "        \n",
    "        # Priority adjustment\n",
    "        priority_multipliers = {'high': 0.8, 'medium': 1.0, 'low': 1.3}\n",
    "        priority_eta = base_eta * priority_multipliers.get(priority, 1.0)\n",
    "        \n",
    "        # Complexity adjustment\n",
    "        complexity = self._analyze_ticket_complexity(row['ticket_text'])\n",
    "        complexity_multipliers = {'simple': 0.8, 'moderate': 1.0, 'complex': 1.4}\n",
    "        final_eta = priority_eta * complexity_multipliers.get(complexity, 1.0)\n",
    "        \n",
    "        return final_eta\n",
    "    \n",
    "    def _analyze_ticket_complexity(self, text):\n",
    "        \"\"\"Analyze ticket complexity using learned patterns\"\"\"\n",
    "        text_lower = str(text).lower()\n",
    "        \n",
    "        # Check against learned complexity patterns\n",
    "        complexity_scores = {}\n",
    "        for complexity, pattern_data in self.complexity_patterns.items():\n",
    "            if complexity != 'category_complexity' and isinstance(pattern_data, dict):\n",
    "                keywords = pattern_data.get('keywords', [])\n",
    "                score = sum(1 for keyword in keywords if keyword in text_lower)\n",
    "                complexity_scores[complexity] = score\n",
    "        \n",
    "        # Determine complexity based on scores\n",
    "        if complexity_scores.get('complex', 0) > 0:\n",
    "            return 'complex'\n",
    "        elif complexity_scores.get('simple', 0) > 0:\n",
    "            return 'simple'\n",
    "        else:\n",
    "            return 'moderate'\n",
    "    \n",
    "    def _calculate_confidence(self, ml_eta, pattern_eta):\n",
    "        \"\"\"Calculate prediction confidence based on agreement\"\"\"\n",
    "        diff = abs(ml_eta - pattern_eta)\n",
    "        max_diff = max(ml_eta, pattern_eta)\n",
    "        \n",
    "        if max_diff == 0:\n",
    "            return 1.0\n",
    "        \n",
    "        disagreement = diff / max_diff\n",
    "        confidence = max(0.5, 1.0 - disagreement)\n",
    "        \n",
    "        return round(confidence, 3)\n",
    "\n",
    "# Initialize advanced ETA predictor\n",
    "print(\"\\nüöÄ Initializing Advanced ETA Predictor...\")\n",
    "advanced_eta_predictor = AdvancedETAPredictor(full_training_data, model_config)\n",
    "\n",
    "# Train ML models\n",
    "X_test, y_test, rf_pred, gb_pred = advanced_eta_predictor.train_ml_models()\n",
    "\n",
    "print(\"\\n‚úÖ Advanced ETA Predictor ready with trained ML models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run advanced ETA predictions\n",
    "print(\"\\nüéØ Running Advanced ETA Predictions...\")\n",
    "\n",
    "start_time = time.time()\n",
    "eta_predictions = advanced_eta_predictor.predict_eta_advanced(classification_results)\n",
    "end_time = time.time()\n",
    "\n",
    "processing_time = end_time - start_time\n",
    "avg_time_per_prediction = processing_time / len(eta_predictions) if len(eta_predictions) > 0 else 0\n",
    "\n",
    "print(f\"\\n‚úÖ Advanced ETA Predictions Complete!\")\n",
    "print(f\"- Processed: {len(eta_predictions)} tickets\")\n",
    "print(f\"- Total time: {processing_time:.2f} seconds\")\n",
    "print(f\"- Avg time per prediction: {avg_time_per_prediction:.3f} seconds\")\n",
    "\n",
    "# Advanced analysis\n",
    "print(f\"\\nüìä Advanced ETA Analysis:\")\n",
    "print(f\"- Average final ETA: {eta_predictions['final_eta'].mean():.1f} hours\")\n",
    "print(f\"- ETA range: {eta_predictions['final_eta'].min():.1f} - {eta_predictions['final_eta'].max():.1f} hours\")\n",
    "print(f\"- ML vs Pattern prediction difference: {abs(eta_predictions['ml_eta'] - eta_predictions['pattern_eta']).mean():.2f} hours avg\")\n",
    "print(f\"- Average prediction confidence: {eta_predictions['confidence'].mean():.3f}\")\n",
    "\n",
    "# Complexity analysis\n",
    "complexity_analysis = eta_predictions.groupby('complexity').agg({\n",
    "    'final_eta': ['count', 'mean', 'std'],\n",
    "    'confidence': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(f\"\\nüîß Complexity-based Analysis:\")\n",
    "for complexity in eta_predictions['complexity'].unique():\n",
    "    comp_data = eta_predictions[eta_predictions['complexity'] == complexity]\n",
    "    print(f\"  {complexity}: {comp_data['final_eta'].mean():.1f}h avg ({len(comp_data)} tickets)\")\n",
    "\n",
    "# Category-based analysis\n",
    "print(f\"\\nüìà Category-based ETA Analysis:\")\n",
    "category_analysis = eta_predictions.groupby('predicted_category').agg({\n",
    "    'final_eta': ['count', 'mean'],\n",
    "    'confidence': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "for category in eta_predictions['predicted_category'].unique():\n",
    "    cat_data = eta_predictions[eta_predictions['predicted_category'] == category]\n",
    "    print(f\"  {category}: {cat_data['final_eta'].mean():.1f}h avg ({len(cat_data)} tickets)\")\n",
    "\n",
    "# Workload analysis\n",
    "total_workload_hours = eta_predictions['final_eta'].sum()\n",
    "tickets_24h = len(eta_predictions[eta_predictions['final_eta'] <= 24])\n",
    "tickets_48h = len(eta_predictions[eta_predictions['final_eta'] <= 48])\n",
    "\n",
    "print(f\"\\n‚ö° Workload Analysis:\")\n",
    "print(f\"- Total workload: {total_workload_hours:.1f} hours\")\n",
    "print(f\"- Resolvable in 24h: {tickets_24h} tickets ({tickets_24h/len(eta_predictions)*100:.1f}%)\")\n",
    "print(f\"- Resolvable in 48h: {tickets_48h} tickets ({tickets_48h/len(eta_predictions)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced visualization\n",
    "print(\"\\nüìä Creating Advanced ETA Visualizations...\")\n",
    "\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "# 1. ETA distribution by complexity\n",
    "plt.subplot(3, 4, 1)\n",
    "eta_predictions.boxplot(column='final_eta', by='complexity', ax=plt.gca())\n",
    "plt.title('ETA Distribution by Complexity')\n",
    "plt.suptitle('')\n",
    "plt.ylabel('Hours')\n",
    "\n",
    "# 2. ML vs Pattern predictions\n",
    "plt.subplot(3, 4, 2)\n",
    "plt.scatter(eta_predictions['ml_eta'], eta_predictions['pattern_eta'], alpha=0.6)\n",
    "plt.plot([0, eta_predictions[['ml_eta', 'pattern_eta']].max().max()], \n",
    "         [0, eta_predictions[['ml_eta', 'pattern_eta']].max().max()], 'r--')\n",
    "plt.xlabel('ML Prediction (hours)')\n",
    "plt.ylabel('Pattern Prediction (hours)')\n",
    "plt.title('ML vs Pattern Predictions')\n",
    "\n",
    "# 3. Confidence distribution\n",
    "plt.subplot(3, 4, 3)\n",
    "plt.hist(eta_predictions['confidence'], bins=20, alpha=0.7, color='green')\n",
    "plt.xlabel('Confidence Score')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Prediction Confidence Distribution')\n",
    "\n",
    "# 4. ETA by category\n",
    "plt.subplot(3, 4, 4)\n",
    "category_means = eta_predictions.groupby('predicted_category')['final_eta'].mean().sort_values()\n",
    "category_means.plot(kind='bar', ax=plt.gca(), color='skyblue')\n",
    "plt.title('Average ETA by Category')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Hours')\n",
    "\n",
    "# 5. ETA by priority\n",
    "plt.subplot(3, 4, 5)\n",
    "eta_predictions.boxplot(column='final_eta', by='predicted_priority', ax=plt.gca())\n",
    "plt.title('ETA Distribution by Priority')\n",
    "plt.suptitle('')\n",
    "plt.ylabel('Hours')\n",
    "\n",
    "# 6. Overall ETA distribution\n",
    "plt.subplot(3, 4, 6)\n",
    "plt.hist(eta_predictions['final_eta'], bins=20, alpha=0.7, color='orange')\n",
    "plt.xlabel('Final ETA (hours)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Overall ETA Distribution')\n",
    "\n",
    "# 7. Workload timeline\n",
    "plt.subplot(3, 4, 7)\n",
    "eta_predictions['completion_date'] = pd.to_datetime(eta_predictions['estimated_completion'])\n",
    "daily_completions = eta_predictions.groupby(eta_predictions['completion_date'].dt.date).size()\n",
    "daily_completions.plot(kind='line', marker='o', ax=plt.gca())\n",
    "plt.title('Predicted Daily Completions')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Tickets')\n",
    "\n",
    "# 8. Confidence vs ETA difference\n",
    "plt.subplot(3, 4, 8)\n",
    "eta_diff = abs(eta_predictions['ml_eta'] - eta_predictions['pattern_eta'])\n",
    "plt.scatter(eta_diff, eta_predictions['confidence'], alpha=0.6, color='red')\n",
    "plt.xlabel('ML-Pattern ETA Difference (hours)')\n",
    "plt.ylabel('Confidence Score')\n",
    "plt.title('Confidence vs Prediction Disagreement')\n",
    "\n",
    "# 9. ML model performance comparison\n",
    "if len(eta_predictions) > 0:\n",
    "    plt.subplot(3, 4, 9)\n",
    "    performance_data = advanced_eta_predictor.model_performance\n",
    "    models = ['Random Forest', 'Gradient Boosting']\n",
    "    mae_scores = [performance_data['random_forest']['mae'], performance_data['gradient_boosting']['mae']]\n",
    "    plt.bar(models, mae_scores, color=['blue', 'green'])\n",
    "    plt.title('ML Model Performance (MAE)')\n",
    "    plt.ylabel('Mean Absolute Error (hours)')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "# 10. Feature importance (if available)\n",
    "plt.subplot(3, 4, 10)\n",
    "if hasattr(advanced_eta_predictor.rf_model, 'feature_importances_'):\n",
    "    feature_names = ['text_length', 'word_count', 'avg_word_length', 'question_marks', \n",
    "                    'exclamation_marks', 'category_encoded', 'priority_encoded', \n",
    "                    'has_urgency', 'has_technical_terms']\n",
    "    importances = advanced_eta_predictor.rf_model.feature_importances_\n",
    "    \n",
    "    # Sort by importance\n",
    "    sorted_idx = np.argsort(importances)[::-1][:5]  # Top 5\n",
    "    plt.bar(range(len(sorted_idx)), importances[sorted_idx])\n",
    "    plt.xticks(range(len(sorted_idx)), [feature_names[i] for i in sorted_idx], rotation=45)\n",
    "    plt.title('Top Feature Importance')\n",
    "    plt.ylabel('Importance Score')\n",
    "\n",
    "# 11. Category vs Complexity heatmap\n",
    "plt.subplot(3, 4, 11)\n",
    "heatmap_data = eta_predictions.pivot_table(\n",
    "    values='final_eta', \n",
    "    index='predicted_category', \n",
    "    columns='complexity', \n",
    "    aggfunc='mean'\n",
    ")\n",
    "sns.heatmap(heatmap_data, annot=True, fmt='.1f', cmap='YlOrRd', cbar_kws={'label': 'Avg ETA (hours)'})\n",
    "plt.title('ETA: Category vs Complexity')\n",
    "plt.xlabel('Complexity')\n",
    "plt.ylabel('Category')\n",
    "\n",
    "# 12. Cumulative workload\n",
    "plt.subplot(3, 4, 12)\n",
    "sorted_etas = eta_predictions['final_eta'].sort_values()\n",
    "cumulative_workload = sorted_etas.cumsum()\n",
    "plt.plot(range(len(cumulative_workload)), cumulative_workload, color='purple', linewidth=2)\n",
    "plt.xlabel('Ticket Number (sorted by ETA)')\n",
    "plt.ylabel('Cumulative Hours')\n",
    "plt.title('Cumulative Workload')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/optimized_eta_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Advanced ETA analysis charts saved to ../outputs/optimized_eta_analysis.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save optimized ETA results with JSON serialization fix\n",
    "def safe_json_conversion(obj):\n",
    "    \"\"\"Convert numpy/pandas types to JSON-serializable types\"\"\"\n",
    "    if isinstance(obj, (np.integer, np.int64, np.int32)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.floating, np.float64, np.float32)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, (pd.Series, pd.DataFrame)):\n",
    "        return obj.to_dict()\n",
    "    elif isinstance(obj, pd.Timestamp):\n",
    "        return obj.isoformat()\n",
    "    elif isinstance(obj, datetime):\n",
    "        return obj.isoformat()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: safe_json_conversion(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [safe_json_conversion(item) for item in obj]\n",
    "    elif hasattr(obj, 'item'):\n",
    "        return obj.item()\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "output_dir = Path(\"../outputs\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Prepare eta_predictions for CSV (remove datetime objects)\n",
    "eta_predictions_csv = eta_predictions.copy()\n",
    "eta_predictions_csv['estimated_completion_str'] = eta_predictions_csv['estimated_completion'].astype(str)\n",
    "eta_predictions_csv = eta_predictions_csv.drop('estimated_completion', axis=1)\n",
    "\n",
    "# Save detailed ETA predictions\n",
    "eta_predictions_csv.to_csv(output_dir / 'optimized_eta_predictions.csv', index=False)\n",
    "\n",
    "# Create comprehensive performance summary\n",
    "performance_summary = {\n",
    "    'total_tickets': len(eta_predictions),\n",
    "    'avg_final_eta': float(eta_predictions['final_eta'].mean()),\n",
    "    'min_eta': float(eta_predictions['final_eta'].min()),\n",
    "    'max_eta': float(eta_predictions['final_eta'].max()),\n",
    "    'std_eta': float(eta_predictions['final_eta'].std()),\n",
    "    'median_eta': float(eta_predictions['final_eta'].median()),\n",
    "    \n",
    "    'ml_model_performance': safe_json_conversion(advanced_eta_predictor.model_performance),\n",
    "    \n",
    "    'workload_analysis': {\n",
    "        'total_workload_hours': float(eta_predictions['final_eta'].sum()),\n",
    "        'tickets_24h': int(len(eta_predictions[eta_predictions['final_eta'] <= 24])),\n",
    "        'tickets_48h': int(len(eta_predictions[eta_predictions['final_eta'] <= 48])),\n",
    "        'tickets_week': int(len(eta_predictions[eta_predictions['final_eta'] <= 168]))\n",
    "    },\n",
    "    \n",
    "    'prediction_quality': {\n",
    "        'avg_confidence': float(eta_predictions['confidence'].mean()),\n",
    "        'high_confidence_predictions': int(len(eta_predictions[eta_predictions['confidence'] > 0.8])),\n",
    "        'avg_ml_pattern_difference': float(abs(eta_predictions['ml_eta'] - eta_predictions['pattern_eta']).mean())\n",
    "    },\n",
    "    \n",
    "    'complexity_analysis': safe_json_conversion(\n",
    "        eta_predictions.groupby('complexity')['final_eta'].agg(['count', 'mean', 'std']).to_dict()\n",
    "    ),\n",
    "    \n",
    "    'category_analysis': safe_json_conversion(\n",
    "        eta_predictions.groupby('predicted_category')['final_eta'].agg(['count', 'mean', 'std']).to_dict()\n",
    "    ),\n",
    "    \n",
    "    'priority_analysis': safe_json_conversion(\n",
    "        eta_predictions.groupby('predicted_priority')['final_eta'].agg(['count', 'mean', 'std']).to_dict()\n",
    "    ),\n",
    "    \n",
    "    'processing_performance': {\n",
    "        'total_processing_time': float(processing_time),\n",
    "        'avg_time_per_prediction': float(avg_time_per_prediction),\n",
    "        'predictions_per_second': float(len(eta_predictions) / processing_time if processing_time > 0 else 0)\n",
    "    },\n",
    "    \n",
    "    'training_data_stats': {\n",
    "        'total_training_samples': len(full_training_data),\n",
    "        'training_eta_range': [float(full_training_data['estimated_hours'].min()), \n",
    "                              float(full_training_data['estimated_hours'].max())],\n",
    "        'training_avg_eta': float(full_training_data['estimated_hours'].mean())\n",
    "    },\n",
    "    \n",
    "    'system_info': {\n",
    "        'model_version': 'optimized_v2.0',\n",
    "        'prediction_method': 'ensemble_ml_pattern',\n",
    "        'optimization_level': 'advanced',\n",
    "        'data_source': '100_percent_real_customer_data'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save with JSON serialization fix\n",
    "performance_summary_safe = safe_json_conversion(performance_summary)\n",
    "\n",
    "with open(output_dir / 'optimized_eta_performance_summary.json', 'w') as f:\n",
    "    json.dump(performance_summary_safe, f, indent=2)\n",
    "\n",
    "print(\"\\nüíæ Optimized ETA Results Saved:\")\n",
    "print(f\"- Detailed predictions: {output_dir}/optimized_eta_predictions.csv\")\n",
    "print(f\"- Performance summary: {output_dir}/optimized_eta_performance_summary.json\")\n",
    "print(f\"- Analysis charts: {output_dir}/optimized_eta_analysis.png\")\n",
    "\n",
    "print(f\"\\nüèÜ OPTIMIZED ETA SYSTEM SUMMARY:\")\n",
    "print(f\"‚úÖ Processed: {performance_summary['total_tickets']:,} tickets\")\n",
    "print(f\"‚úÖ ML Model Performance: RF MAE {advanced_eta_predictor.model_performance['random_forest']['mae']:.2f}h, R¬≤ {advanced_eta_predictor.model_performance['random_forest']['r2']:.3f}\")\n",
    "print(f\"‚úÖ Average ETA: {performance_summary['avg_final_eta']:.1f} hours\")\n",
    "print(f\"‚úÖ Prediction Confidence: {performance_summary['prediction_quality']['avg_confidence']:.3f}\")\n",
    "print(f\"‚úÖ Total Workload: {performance_summary['workload_analysis']['total_workload_hours']:.1f} hours\")\n",
    "print(f\"‚úÖ 24h Resolvable: {performance_summary['workload_analysis']['tickets_24h']} tickets\")\n",
    "print(f\"‚úÖ Training Data: {performance_summary['training_data_stats']['total_training_samples']:,} real samples\")\n",
    "print(f\"‚úÖ Processing Speed: {performance_summary['processing_performance']['predictions_per_second']:.1f} predictions/second\")\n",
    "\n",
    "print(f\"\\nüéâ OPTIMIZED ETA Prediction System Complete!\")\n",
    "print(\"Ready to proceed to optimized notebook 05 (Sentiment Analysis & Response Generation)\")\n",
    "\n",
    "# Memory cleanup\n",
    "gc.collect()\n",
    "print(\"üßπ Memory cleaned up\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}