{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-collection-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "import zipfile\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset-download",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_twitter_dataset():\n",
    "    twitter_url = \"https://www.kaggle.com/api/v1/datasets/download/thoughtvector/customer-support-on-twitter\"\n",
    "    if not os.path.exists('../data/raw'):\n",
    "        os.makedirs('../data/raw')\n",
    "    \n",
    "    if not os.path.exists('../data/raw/customer_support_twitter.csv'):\n",
    "        response = requests.get(twitter_url)\n",
    "        with open('../data/raw/twitter_data.zip', 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        \n",
    "        with zipfile.ZipFile('../data/raw/twitter_data.zip', 'r') as zip_ref:\n",
    "            zip_ref.extractall('../data/raw')\n",
    "    \n",
    "    return pd.read_csv('../data/raw/twcs.csv')\n",
    "\n",
    "def load_bitext_dataset():\n",
    "    dataset = load_dataset(\"bitext/Bitext-customer-support-llm-chatbot-training-dataset\")\n",
    "    return pd.DataFrame(dataset['train'])\n",
    "\n",
    "def create_synthetic_data():\n",
    "    categories = ['billing', 'technical', 'general_inquiry', 'complaint', 'compliment', 'account']\n",
    "    priorities = ['high', 'medium', 'low']\n",
    "    \n",
    "    synthetic_texts = [\n",
    "        \"My internet connection keeps dropping every few minutes\",\n",
    "        \"I was charged twice for my monthly subscription\",\n",
    "        \"How do I reset my password?\",\n",
    "        \"Your service is terrible, I want a refund\",\n",
    "        \"Great customer service, thank you for the help\",\n",
    "        \"I need to update my billing address\",\n",
    "        \"The app crashes when I try to log in\",\n",
    "        \"Can you explain the new pricing structure?\",\n",
    "        \"My account has been suspended without notice\",\n",
    "        \"Unable to access my dashboard\"\n",
    "    ]\n",
    "    \n",
    "    synthetic_data = []\n",
    "    for i, text in enumerate(synthetic_texts):\n",
    "        synthetic_data.append({\n",
    "            'ticket_id': f'SYNTH_{i+1:03d}',\n",
    "            'text': text,\n",
    "            'category': np.random.choice(categories),\n",
    "            'priority': np.random.choice(priorities),\n",
    "            'estimated_hours': np.random.uniform(0.5, 48.0)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(synthetic_data)\n",
    "\n",
    "twitter_df = download_twitter_dataset()\n",
    "bitext_df = load_bitext_dataset()\n",
    "synthetic_df = create_synthetic_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-preprocessing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip().lower()\n",
    "\n",
    "def standardize_twitter_data(df):\n",
    "    df_clean = df[df['text'].notna()].copy()\n",
    "    df_clean['text'] = df_clean['text'].apply(clean_text)\n",
    "    df_clean = df_clean[df_clean['text'].str.len() > 10]\n",
    "    \n",
    "    df_clean['ticket_id'] = 'TW_' + df_clean.index.astype(str)\n",
    "    df_clean['category'] = 'general_inquiry'\n",
    "    df_clean['priority'] = np.random.choice(['high', 'medium', 'low'], size=len(df_clean))\n",
    "    df_clean['estimated_hours'] = np.random.uniform(1, 24, size=len(df_clean))\n",
    "    \n",
    "    return df_clean[['ticket_id', 'text', 'category', 'priority', 'estimated_hours']]\n",
    "\n",
    "def standardize_bitext_data(df):\n",
    "    df_clean = df.copy()\n",
    "    df_clean['text'] = df_clean['instruction'].apply(clean_text)\n",
    "    df_clean = df_clean[df_clean['text'].str.len() > 10]\n",
    "    \n",
    "    df_clean['ticket_id'] = 'BT_' + df_clean.index.astype(str)\n",
    "    \n",
    "    category_mapping = {\n",
    "        'account': 'account',\n",
    "        'billing': 'billing',\n",
    "        'technical': 'technical',\n",
    "        'general': 'general_inquiry'\n",
    "    }\n",
    "    \n",
    "    df_clean['category'] = df_clean.get('category', 'general_inquiry')\n",
    "    df_clean['priority'] = np.random.choice(['high', 'medium', 'low'], size=len(df_clean))\n",
    "    df_clean['estimated_hours'] = np.random.uniform(0.5, 12, size=len(df_clean))\n",
    "    \n",
    "    return df_clean[['ticket_id', 'text', 'category', 'priority', 'estimated_hours']]\n",
    "\n",
    "twitter_standardized = standardize_twitter_data(twitter_df.head(1000))\n",
    "bitext_standardized = standardize_bitext_data(bitext_df.head(500))\n",
    "synthetic_standardized = synthetic_df.copy()\n",
    "\n",
    "combined_df = pd.concat([\n",
    "    twitter_standardized,\n",
    "    bitext_standardized,\n",
    "    synthetic_standardized\n",
    "], ignore_index=True)\n",
    "\n",
    "combined_df = combined_df.drop_duplicates(subset=['text']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-test-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, temp_df = train_test_split(combined_df, test_size=0.3, random_state=42, stratify=combined_df['category'])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['category'])\n",
    "\n",
    "if not os.path.exists('../data/processed'):\n",
    "    os.makedirs('../data/processed')\n",
    "\n",
    "train_df.to_csv('../data/processed/train_data.csv', index=False)\n",
    "val_df.to_csv('../data/processed/val_data.csv', index=False)\n",
    "test_df.to_csv('../data/processed/test_data.csv', index=False)\n",
    "combined_df.to_csv('../data/processed/full_dataset.csv', index=False)\n",
    "\n",
    "dataset_stats = {\n",
    "    'total_samples': len(combined_df),\n",
    "    'train_samples': len(train_df),\n",
    "    'val_samples': len(val_df),\n",
    "    'test_samples': len(test_df),\n",
    "    'categories': combined_df['category'].value_counts().to_dict(),\n",
    "    'priority_distribution': combined_df['priority'].value_counts().to_dict()\n",
    "}\n",
    "\n",
    "with open('../data/processed/dataset_stats.json', 'w') as f:\n",
    "    json.dump(dataset_stats, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}