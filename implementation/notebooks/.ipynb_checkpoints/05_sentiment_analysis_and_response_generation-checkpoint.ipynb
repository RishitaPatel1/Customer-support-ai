{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Sentiment Analysis and Response Generation\n",
    "\n",
    "This notebook implements advanced sentiment analysis and LLaMA-powered response generation.\n",
    "It uses the classification and ETA results from previous notebooks to generate contextual responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=== Sentiment Analysis & Response Generation ===\")\n",
    "print(\"LLaMA-powered sentiment analysis and personalized response generation\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previous results and configuration\n",
    "def load_previous_results():\n",
    "    \"\"\"Load ETA predictions and model configuration\"\"\"\n",
    "    \n",
    "    # Load ETA predictions\n",
    "    eta_path = Path(\"../outputs/eta_predictions.csv\")\n",
    "    if eta_path.exists():\n",
    "        eta_df = pd.read_csv(eta_path)\n",
    "        print(f\"‚úÖ Loaded {len(eta_df)} tickets with ETA predictions\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"ETA predictions not found. Please run notebook 04 first.\")\n",
    "    \n",
    "    # Load model configuration\n",
    "    config_path = Path(\"../outputs/customer_support_model_config.json\")\n",
    "    if config_path.exists():\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = json.load(f)\n",
    "        print(\"‚úÖ Model configuration loaded\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Model configuration not found. Please run notebook 02 first.\")\n",
    "    \n",
    "    # Load LLaMA setup config\n",
    "    llama_config_path = Path(\"../outputs/llama_setup_config.json\")\n",
    "    if llama_config_path.exists():\n",
    "        with open(llama_config_path, 'r') as f:\n",
    "            llama_config = json.load(f)\n",
    "        print(\"‚úÖ LLaMA setup configuration loaded\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"LLaMA configuration not found. Please run LLAMA_SETUP.ipynb first.\")\n",
    "    \n",
    "    return eta_df, config, llama_config\n",
    "\n",
    "eta_results, model_config, llama_config = load_previous_results()\n",
    "print(f\"\\nReady to process {len(eta_results)} tickets for sentiment analysis and response generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Sentiment Analysis System\n",
    "class AdvancedSentimentAnalyzer:\n",
    "    def __init__(self):\n",
    "        # Enhanced sentiment indicators\n",
    "        self.sentiment_patterns = {\n",
    "            'very_positive': {\n",
    "                'words': ['amazing', 'excellent', 'outstanding', 'fantastic', 'perfect', 'love'],\n",
    "                'phrases': ['thank you so much', 'really appreciate', 'exceeded expectations'],\n",
    "                'score': 2.0\n",
    "            },\n",
    "            'positive': {\n",
    "                'words': ['good', 'great', 'nice', 'helpful', 'thanks', 'satisfied'],\n",
    "                'phrases': ['thank you', 'well done', 'good job'],\n",
    "                'score': 1.0\n",
    "            },\n",
    "            'neutral': {\n",
    "                'words': ['okay', 'fine', 'acceptable', 'standard'],\n",
    "                'phrases': ['how to', 'can you', 'please help'],\n",
    "                'score': 0.0\n",
    "            },\n",
    "            'negative': {\n",
    "                'words': ['bad', 'poor', 'disappointed', 'frustrated', 'annoyed', 'problem'],\n",
    "                'phrases': ['not working', 'not satisfied', 'having issues'],\n",
    "                'score': -1.0\n",
    "            },\n",
    "            'very_negative': {\n",
    "                'words': ['terrible', 'awful', 'horrible', 'worst', 'hate', 'furious'],\n",
    "                'phrases': ['completely unacceptable', 'want my money back', 'never again'],\n",
    "                'score': -2.0\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def analyze_sentiment_detailed(self, text):\n",
    "        \"\"\"Perform detailed sentiment analysis\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        sentiment_score = 0.0\n",
    "        matched_patterns = []\n",
    "        \n",
    "        # Check each sentiment category\n",
    "        for sentiment_type, patterns in self.sentiment_patterns.items():\n",
    "            # Check words\n",
    "            word_matches = sum(1 for word in patterns['words'] if word in text_lower)\n",
    "            \n",
    "            # Check phrases\n",
    "            phrase_matches = sum(1 for phrase in patterns['phrases'] if phrase in text_lower)\n",
    "            \n",
    "            # Calculate contribution\n",
    "            total_matches = word_matches + phrase_matches * 2  # Phrases count double\n",
    "            if total_matches > 0:\n",
    "                sentiment_score += patterns['score'] * total_matches\n",
    "                matched_patterns.append({\n",
    "                    'type': sentiment_type,\n",
    "                    'matches': total_matches,\n",
    "                    'contribution': patterns['score'] * total_matches\n",
    "                })\n",
    "        \n",
    "        # Determine final sentiment\n",
    "        if sentiment_score >= 2.0:\n",
    "            final_sentiment = 'very_positive'\n",
    "        elif sentiment_score >= 0.5:\n",
    "            final_sentiment = 'positive'\n",
    "        elif sentiment_score <= -2.0:\n",
    "            final_sentiment = 'very_negative'\n",
    "        elif sentiment_score <= -0.5:\n",
    "            final_sentiment = 'negative'\n",
    "        else:\n",
    "            final_sentiment = 'neutral'\n",
    "        \n",
    "        return {\n",
    "            'sentiment': final_sentiment,\n",
    "            'confidence_score': abs(sentiment_score),\n",
    "            'raw_score': sentiment_score,\n",
    "            'matched_patterns': matched_patterns\n",
    "        }\n",
    "\n",
    "print(\"Initializing advanced sentiment analyzer...\")\n",
    "sentiment_analyzer = AdvancedSentimentAnalyzer()\n",
    "print(\"‚úÖ Sentiment analyzer ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLaMA Response Generator\n",
    "class LLaMAResponseGenerator:\n",
    "    def __init__(self, llama_config):\n",
    "        self.llama_config = llama_config\n",
    "        self.model_name = llama_config['model_name']\n",
    "        self.device = llama_config['system_specs']['device']\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        \n",
    "        # Response templates based on sentiment and category\n",
    "        self.response_templates = {\n",
    "            'very_positive': {\n",
    "                'greeting': \"Thank you so much for your wonderful feedback!\",\n",
    "                'tone': 'enthusiastic'\n",
    "            },\n",
    "            'positive': {\n",
    "                'greeting': \"Thank you for contacting us!\",\n",
    "                'tone': 'friendly'\n",
    "            },\n",
    "            'neutral': {\n",
    "                'greeting': \"Thank you for reaching out to us.\",\n",
    "                'tone': 'professional'\n",
    "            },\n",
    "            'negative': {\n",
    "                'greeting': \"We apologize for the inconvenience you've experienced.\",\n",
    "                'tone': 'empathetic'\n",
    "            },\n",
    "            'very_negative': {\n",
    "                'greeting': \"We sincerely apologize for the frustrating experience you've had.\",\n",
    "                'tone': 'very_empathetic'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def setup_model(self):\n",
    "        \"\"\"Setup LLaMA model for response generation\"\"\"\n",
    "        print(f\"Loading LLaMA model: {self.model_name}\")\n",
    "        \n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Load model\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            dtype=torch.float32,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        print(\"‚úÖ LLaMA response generator ready\")\n",
    "    \n",
    "    def generate_response(self, ticket_data):\n",
    "        \"\"\"Generate personalized response based on ticket data\"\"\"\n",
    "        ticket_text = ticket_data['ticket_text']\n",
    "        category = ticket_data['category']\n",
    "        priority = ticket_data['priority']\n",
    "        sentiment = ticket_data['detailed_sentiment']['sentiment']\n",
    "        eta = ticket_data['predicted_eta']\n",
    "        \n",
    "        # Get response template\n",
    "        template = self.response_templates[sentiment]\n",
    "        greeting = template['greeting']\n",
    "        tone = template['tone']\n",
    "        \n",
    "        # Create context-aware prompt\n",
    "        prompt = f\"\"\"<|system|>\n",
    "You are a professional customer support representative. Generate a {tone} response that addresses the customer's specific concern.\n",
    "\n",
    "<|user|>\n",
    "Customer Message: {ticket_text}\n",
    "\n",
    "Context:\n",
    "- Issue Category: {category}\n",
    "- Priority: {priority} \n",
    "- Customer Sentiment: {sentiment}\n",
    "- Estimated Resolution: {eta} hours\n",
    "\n",
    "Write a personalized response that:\n",
    "1. Acknowledges their specific issue\n",
    "2. Shows appropriate empathy for their sentiment\n",
    "3. Provides helpful next steps\n",
    "4. Sets proper expectations\n",
    "\n",
    "<|assistant|>\n",
    "{greeting} \"\"\"\n",
    "        \n",
    "        # Generate response\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", max_length=400, truncation=True)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                inputs['input_ids'],\n",
    "                max_new_tokens=120,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        generated_part = response.split(greeting)[-1].strip()\n",
    "        \n",
    "        return greeting + \" \" + generated_part\n",
    "\n",
    "print(\"Initializing LLaMA response generator...\")\n",
    "response_generator = LLaMAResponseGenerator(llama_config)\n",
    "response_generator.setup_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all tickets for detailed sentiment analysis\n",
    "print(\"Performing detailed sentiment analysis on all tickets...\")\n",
    "\n",
    "detailed_sentiment_results = []\n",
    "\n",
    "for index, row in eta_results.iterrows():\n",
    "    ticket_text = row['ticket_text']\n",
    "    \n",
    "    # Perform detailed sentiment analysis\n",
    "    sentiment_analysis = sentiment_analyzer.analyze_sentiment_detailed(ticket_text)\n",
    "    \n",
    "    # Prepare data for response generation\n",
    "    ticket_data = {\n",
    "        'ticket_text': ticket_text,\n",
    "        'category': row['category'],\n",
    "        'priority': row['priority'],\n",
    "        'predicted_eta': row['predicted_eta'],\n",
    "        'complexity': row['complexity'],\n",
    "        'detailed_sentiment': sentiment_analysis\n",
    "    }\n",
    "    \n",
    "    detailed_sentiment_results.append(ticket_data)\n",
    "\n",
    "print(f\"\\n‚úÖ Detailed sentiment analysis complete for {len(detailed_sentiment_results)} tickets\")\n",
    "\n",
    "# Show sentiment distribution\n",
    "sentiment_counts = {}\n",
    "for result in detailed_sentiment_results:\n",
    "    sentiment = result['detailed_sentiment']['sentiment']\n",
    "    sentiment_counts[sentiment] = sentiment_counts.get(sentiment, 0) + 1\n",
    "\n",
    "print(\"\\nüòä Detailed Sentiment Distribution:\")\n",
    "for sentiment, count in sorted(sentiment_counts.items()):\n",
    "    print(f\"  {sentiment}: {count} tickets\")\n",
    "\n",
    "# Calculate average confidence scores\n",
    "avg_confidence = np.mean([r['detailed_sentiment']['confidence_score'] for r in detailed_sentiment_results])\n",
    "print(f\"\\nüìä Average sentiment confidence: {avg_confidence:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate personalized responses for a sample of tickets\n",
    "print(\"Generating personalized responses using LLaMA...\")\n",
    "\n",
    "# Select diverse sample for response generation (to save time)\n",
    "sample_tickets = []\n",
    "for sentiment in ['very_positive', 'positive', 'neutral', 'negative', 'very_negative']:\n",
    "    sentiment_tickets = [t for t in detailed_sentiment_results if t['detailed_sentiment']['sentiment'] == sentiment]\n",
    "    if sentiment_tickets:\n",
    "        sample_tickets.extend(sentiment_tickets[:2])  # 2 from each sentiment\n",
    "\n",
    "print(f\"Generating responses for {len(sample_tickets)} diverse tickets...\")\n",
    "\n",
    "response_results = []\n",
    "\n",
    "for i, ticket_data in enumerate(sample_tickets, 1):\n",
    "    print(f\"Generating response {i}/{len(sample_tickets)}...\")\n",
    "    \n",
    "    try:\n",
    "        generated_response = response_generator.generate_response(ticket_data)\n",
    "        \n",
    "        result = {\n",
    "            'ticket_text': ticket_data['ticket_text'][:100] + \"...\" if len(ticket_data['ticket_text']) > 100 else ticket_data['ticket_text'],\n",
    "            'category': ticket_data['category'],\n",
    "            'priority': ticket_data['priority'],\n",
    "            'sentiment': ticket_data['detailed_sentiment']['sentiment'],\n",
    "            'confidence': ticket_data['detailed_sentiment']['confidence_score'],\n",
    "            'eta_hours': ticket_data['predicted_eta'],\n",
    "            'generated_response': generated_response[:300] + \"...\" if len(generated_response) > 300 else generated_response\n",
    "        }\n",
    "        \n",
    "        response_results.append(result)\n",
    "        \n",
    "        print(f\"‚úÖ Response generated for {ticket_data['detailed_sentiment']['sentiment']} sentiment ticket\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generating response: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n‚úÖ Response generation complete! Generated {len(response_results)} personalized responses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample responses\n",
    "print(\"\\nüìù Sample Generated Responses:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, response in enumerate(response_results[:5], 1):\n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"Ticket: {response['ticket_text']}\")\n",
    "    print(f\"Category: {response['category']} | Priority: {response['priority']} | Sentiment: {response['sentiment']}\")\n",
    "    print(f\"ETA: {response['eta_hours']} hours\")\n",
    "    print(f\"Response: {response['generated_response']}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(\"\\nüìä Response Quality Metrics:\")\n",
    "print(f\"- Total responses generated: {len(response_results)}\")\n",
    "print(f\"- Average response length: {np.mean([len(r['generated_response']) for r in response_results]):.0f} characters\")\n",
    "print(f\"- Sentiment coverage: {len(set(r['sentiment'] for r in response_results))} different sentiments\")\n",
    "print(f\"- Category coverage: {len(set(r['category'] for r in response_results))} different categories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentiment analysis visualization\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Sentiment distribution\n",
    "plt.subplot(2, 3, 1)\n",
    "sentiment_df = pd.DataFrame([(r['detailed_sentiment']['sentiment'], 1) for r in detailed_sentiment_results], \n",
    "                          columns=['sentiment', 'count'])\n",
    "sentiment_counts = sentiment_df['sentiment'].value_counts()\n",
    "plt.pie(sentiment_counts.values, labels=sentiment_counts.index, autopct='%1.1f%%')\n",
    "plt.title('Sentiment Distribution')\n",
    "\n",
    "# Confidence scores by sentiment\n",
    "plt.subplot(2, 3, 2)\n",
    "confidence_data = [(r['detailed_sentiment']['sentiment'], r['detailed_sentiment']['confidence_score']) \n",
    "                  for r in detailed_sentiment_results]\n",
    "confidence_df = pd.DataFrame(confidence_data, columns=['sentiment', 'confidence'])\n",
    "confidence_df.boxplot(column='confidence', by='sentiment', ax=plt.gca())\n",
    "plt.title('Confidence Scores by Sentiment')\n",
    "plt.suptitle('')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Sentiment vs Category heatmap\n",
    "plt.subplot(2, 3, 3)\n",
    "category_sentiment = [(r['category'], r['detailed_sentiment']['sentiment']) for r in detailed_sentiment_results]\n",
    "cs_df = pd.DataFrame(category_sentiment, columns=['category', 'sentiment'])\n",
    "heatmap_data = pd.crosstab(cs_df['category'], cs_df['sentiment'])\n",
    "sns.heatmap(heatmap_data, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Category vs Sentiment')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "# Sentiment vs Priority\n",
    "plt.subplot(2, 3, 4)\n",
    "priority_sentiment = [(r['priority'], r['detailed_sentiment']['sentiment']) for r in detailed_sentiment_results]\n",
    "ps_df = pd.DataFrame(priority_sentiment, columns=['priority', 'sentiment'])\n",
    "ps_counts = pd.crosstab(ps_df['priority'], ps_df['sentiment'])\n",
    "ps_counts.plot(kind='bar', stacked=True, ax=plt.gca())\n",
    "plt.title('Priority vs Sentiment')\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(title='Sentiment', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Response length distribution\n",
    "plt.subplot(2, 3, 5)\n",
    "response_lengths = [len(r['generated_response']) for r in response_results]\n",
    "plt.hist(response_lengths, bins=10, alpha=0.7, color='lightgreen')\n",
    "plt.title('Generated Response Lengths')\n",
    "plt.xlabel('Characters')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Sentiment confidence histogram\n",
    "plt.subplot(2, 3, 6)\n",
    "all_confidences = [r['detailed_sentiment']['confidence_score'] for r in detailed_sentiment_results]\n",
    "plt.hist(all_confidences, bins=15, alpha=0.7, color='orange')\n",
    "plt.title('Sentiment Confidence Distribution')\n",
    "plt.xlabel('Confidence Score')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/sentiment_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Sentiment analysis charts saved to ../outputs/sentiment_analysis.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results\n",
    "output_dir = Path(\"../outputs\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save detailed sentiment analysis results\n",
    "sentiment_export = []\n",
    "for result in detailed_sentiment_results:\n",
    "    export_item = {\n",
    "        'ticket_text': result['ticket_text'],\n",
    "        'category': result['category'],\n",
    "        'priority': result['priority'],\n",
    "        'predicted_eta': result['predicted_eta'],\n",
    "        'complexity': result['complexity'],\n",
    "        'sentiment': result['detailed_sentiment']['sentiment'],\n",
    "        'confidence_score': result['detailed_sentiment']['confidence_score'],\n",
    "        'raw_sentiment_score': result['detailed_sentiment']['raw_score']\n",
    "    }\n",
    "    sentiment_export.append(export_item)\n",
    "\n",
    "sentiment_df = pd.DataFrame(sentiment_export)\n",
    "sentiment_df.to_csv(output_dir / 'detailed_sentiment_analysis.csv', index=False)\n",
    "\n",
    "# Save generated responses\n",
    "responses_df = pd.DataFrame(response_results)\n",
    "responses_df.to_csv(output_dir / 'generated_responses.csv', index=False)\n",
    "\n",
    "# Save summary statistics\n",
    "sentiment_summary = {\n",
    "    'total_tickets_analyzed': len(detailed_sentiment_results),\n",
    "    'responses_generated': len(response_results),\n",
    "    'sentiment_distribution': dict(sentiment_df['sentiment'].value_counts()),\n",
    "    'avg_confidence_score': float(sentiment_df['confidence_score'].mean()),\n",
    "    'sentiment_by_category': sentiment_df.groupby('category')['sentiment'].apply(lambda x: x.mode().iloc[0] if not x.empty else 'neutral').to_dict(),\n",
    "    'avg_response_length': float(np.mean([len(r['generated_response']) for r in response_results])),\n",
    "    'processing_system': 'LLaMA-powered advanced sentiment analysis'\n",
    "}\n",
    "\n",
    "with open(output_dir / 'sentiment_response_summary.json', 'w') as f:\n",
    "    json.dump(sentiment_summary, f, indent=2)\n",
    "\n",
    "print(\"üíæ Results saved:\")\n",
    "print(f\"- Detailed sentiment analysis: {output_dir}/detailed_sentiment_analysis.csv\")\n",
    "print(f\"- Generated responses: {output_dir}/generated_responses.csv\")\n",
    "print(f\"- Summary statistics: {output_dir}/sentiment_response_summary.json\")\n",
    "print(f\"- Analysis charts: {output_dir}/sentiment_analysis.png\")\n",
    "\n",
    "print(\"\\nüéØ Key Achievements:\")\n",
    "print(f\"- Analyzed sentiment for {sentiment_summary['total_tickets_analyzed']} tickets\")\n",
    "print(f\"- Generated {sentiment_summary['responses_generated']} personalized responses\")\n",
    "print(f\"- Average sentiment confidence: {sentiment_summary['avg_confidence_score']:.2f}\")\n",
    "print(f\"- Average response length: {sentiment_summary['avg_response_length']:.0f} characters\")\n",
    "\n",
    "print(\"\\nüéâ Sentiment Analysis & Response Generation Complete!\")\n",
    "print(\"Ready to proceed to notebook 06 (End-to-End Pipeline)\")\n",
    "\n",
    "# Clean up memory\n",
    "del response_generator.model\n",
    "del response_generator.tokenizer\n",
    "import gc\n",
    "gc.collect()\n",
    "print(\"üßπ Memory cleaned up\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}