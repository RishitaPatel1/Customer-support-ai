{
 "cells": [
  {
   "cell_type": "code",
   "id": "model-setup",
   "metadata": {},
   "outputs": [],
   "source": "# Customer Support Intelligence System - Model Setup with LLaMA\n# Implementation based on project proposal specifications\n\nimport os\nimport sys\nimport json\nimport pandas as pd\nimport numpy as np\nimport warnings\nfrom pathlib import Path\n\n# Add src directory to path for imports\nsys.path.append('../src')\n\n# Import our custom LLaMA integration\nfrom llama_integration import LlamaCustomerSupportModel, download_llama_model\n\n# Standard imports\nfrom sentence_transformers import SentenceTransformer\nimport torch\nfrom huggingface_hub import hf_hub_download\n\nwarnings.filterwarnings('ignore')\n\nprint(\"=== Customer Support Intelligence System ===\")\nprint(\"LLaMA-based Ticket Processing Pipeline\")\nprint(\"Based on project proposal specifications\")\nprint()"
  },
  {
   "cell_type": "code",
   "id": "tjz8g2ybr2e",
   "source": "# Method 1: Run Python files directly in Jupyter\n# You can run any .py file using these methods:\n\nprint(\"üîß How to run Python files in Jupyter:\")\nprint(\"1. Magic command: %run ../setup_llama.py\")\nprint(\"2. Execute command: exec(open('../setup_llama.py').read())\")\nprint(\"3. Import as module: import sys; sys.path.append('..'); import setup_llama\")\nprint(\"4. Shell command: !python ../setup_llama.py\")\nprint()\n\n# Method 2: Integrated setup (recommended)\nprint(\"üöÄ Running integrated LLaMA setup...\")\n\n# We'll use the integrated approach below instead of external .py file",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "model-configuration",
   "metadata": {},
   "outputs": [],
   "source": "# Step 1: System Requirements Check\nimport psutil\n\ndef check_system_requirements():\n    \"\"\"Check if system can handle LLaMA model\"\"\"\n    total_ram = psutil.virtual_memory().total / (1024**3)  # GB\n    available_ram = psutil.virtual_memory().available / (1024**3)  # GB\n    \n    print(f\"System Information:\")\n    print(f\"- Total RAM: {total_ram:.1f} GB\")\n    print(f\"- Available RAM: {available_ram:.1f} GB\")\n    print(f\"- PyTorch CUDA Available: {torch.cuda.is_available()}\")\n    \n    if torch.cuda.is_available():\n        print(f\"- CUDA Device: {torch.cuda.get_device_name(0)}\")\n        print(f\"- CUDA Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n    \n    print()\n    \n    # Recommendations\n    if available_ram >= 16:\n        print(\"‚úÖ System suitable for LLaMA 13B model\")\n        return \"13B\"\n    elif available_ram >= 8:\n        print(\"‚úÖ System suitable for LLaMA 7B model\") \n        return \"7B\"\n    else:\n        print(\"‚ö†Ô∏è  Limited RAM - will use fallback model\")\n        return \"fallback\"\n\nrecommended_model = check_system_requirements()"
  },
  {
   "cell_type": "code",
   "id": "test-model",
   "metadata": {},
   "outputs": [],
   "source": "# Step 2: Download and Setup LLaMA Model\n\ndef setup_llama_model(model_size=\"7B\"):\n    \"\"\"Download and setup LLaMA model\"\"\"\n    \n    model_dir = Path(\"../models/llama\")\n    model_dir.mkdir(parents=True, exist_ok=True)\n    \n    # Check if model already exists\n    model_files = {\n        \"7B\": \"llama-2-7b-chat.Q4_K_M.gguf\",\n        \"13B\": \"llama-2-13b-chat.Q6_K.gguf\"\n    }\n    \n    model_file = model_files.get(model_size, model_files[\"7B\"])\n    model_path = model_dir / model_file\n    \n    if model_path.exists():\n        print(f\"‚úÖ LLaMA {model_size} model already exists at {model_path}\")\n        return str(model_path)\n    \n    print(f\"‚¨áÔ∏è  Downloading LLaMA {model_size} model...\")\n    print(\"This may take 10-30 minutes depending on internet speed...\")\n    \n    try:\n        downloaded_path = download_llama_model(model_size)\n        print(f\"‚úÖ Model downloaded successfully!\")\n        return downloaded_path\n    except Exception as e:\n        print(f\"‚ùå Error downloading model: {e}\")\n        print(\"Will use fallback model instead.\")\n        return None\n\n# Download model if system supports it\nmodel_path = None\nif recommended_model != \"fallback\":\n    model_path = setup_llama_model(recommended_model)\n\nprint(f\"Model path: {model_path}\")"
  },
  {
   "cell_type": "code",
   "id": "q5q934gd7j",
   "source": "# Step 5: Save Model Configuration and Test Results\n\n# Create outputs directory\noutput_dir = Path(\"../outputs\")\noutput_dir.mkdir(exist_ok=True)\n\n# Save model configuration\nmodel_config = {\n    'model_type': llama_model.model_type,\n    'model_path': model_path,\n    'config': {\n        'max_tokens': llama_model.config.max_tokens,\n        'temperature': llama_model.config.temperature,\n        'top_p': llama_model.config.top_p,\n        'top_k': llama_model.config.top_k,\n        'repeat_penalty': llama_model.config.repeat_penalty\n    },\n    'categories': llama_model.categories,\n    'priority_levels': llama_model.priority_levels,\n    'sentiment_types': llama_model.sentiment_types,\n    'embedding_dimension': 384,  # SentenceTransformer all-MiniLM-L6-v2\n    'test_tickets_count': len(sample_tickets)\n}\n\nwith open(output_dir / 'llama_model_config.json', 'w') as f:\n    json.dump(model_config, f, indent=2)\n\n# Save test results (without embeddings for readability)\nsimplified_results = []\nfor i, result in enumerate(test_results):\n    if result:\n        simplified_result = {\n            'ticket_id': i + 1,\n            'ticket_text': result['ticket_text'],\n            'classification': result['classification'],\n            'generated_response': result['generated_response'],\n            'model_type': result['model_type']\n        }\n        simplified_results.append(simplified_result)\n\nwith open(output_dir / 'llama_test_results.json', 'w') as f:\n    json.dump(simplified_results, f, indent=2)\n\n# Create summary DataFrame for analysis\nsummary_data = []\nfor result in simplified_results:\n    summary_data.append({\n        'ticket_text': result['ticket_text'],\n        'category': result['classification']['category'],\n        'priority': result['classification']['priority'],\n        'eta_hours': result['classification']['estimated_hours'],\n        'sentiment': result['classification']['sentiment'],\n        'tags': ', '.join(result['classification']['tags'])\n    })\n\ntest_summary_df = pd.DataFrame(summary_data)\ntest_summary_df.to_csv(output_dir / 'llama_test_summary.csv', index=False)\n\nprint(\"üíæ Results saved:\")\nprint(f\"- Model config: {output_dir}/llama_model_config.json\")\nprint(f\"- Test results: {output_dir}/llama_test_results.json\") \nprint(f\"- Summary CSV: {output_dir}/llama_test_summary.csv\")\n\n# Display summary statistics\nprint(f\"\\nüìä Test Summary:\")\nprint(f\"- Total tickets processed: {len(test_summary_df)}\")\nprint(f\"- Categories distribution:\")\nfor category in test_summary_df['category'].value_counts().items():\n    print(f\"  ‚Ä¢ {category[0]}: {category[1]} tickets\")\nprint(f\"- Priority distribution:\")\nfor priority in test_summary_df['priority'].value_counts().items():\n    print(f\"  ‚Ä¢ {priority[0]}: {priority[1]} tickets\")\nprint(f\"- Sentiment distribution:\")\nfor sentiment in test_summary_df['sentiment'].value_counts().items():\n    print(f\"  ‚Ä¢ {sentiment[0]}: {sentiment[1]} tickets\")\n\nprint(\"\\nüéâ LLaMA Model Setup Complete!\")\nprint(\"The model is ready for use in the next notebooks.\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "etsz9lxbzhl",
   "source": "# Step 4: Test LLaMA Model with Sample Tickets\n\n# Test tickets based on project proposal examples\nsample_tickets = [\n    \"My billing statement shows duplicate charges for this month\",\n    \"The application crashes every time I try to upload a file\", \n    \"I love your new feature update, great work!\",\n    \"How do I change my password?\",\n    \"This service is completely unreliable, I want my money back\",\n    \"I need help setting up my account for the first time\",\n    \"The website is loading very slowly today\",\n    \"Thank you for the quick resolution of my previous issue\"\n]\n\nprint(\"üß™ Testing LLaMA Model with Sample Tickets...\")\nprint(\"=\" * 60)\n\ntest_results = []\n\nfor i, ticket in enumerate(sample_tickets, 1):\n    print(f\"\\nüìù Test {i}/8: Processing ticket...\")\n    print(f\"Ticket: {ticket[:50]}...\" if len(ticket) > 50 else f\"Ticket: {ticket}\")\n    \n    try:\n        # Process single ticket through the pipeline\n        result = llama_model.process_single_ticket(ticket)\n        \n        # Display results\n        classification = result['classification']\n        print(f\"‚úÖ Category: {classification['category']}\")\n        print(f\"‚ö° Priority: {classification['priority']}\")\n        print(f\"‚è±Ô∏è  ETA: {classification['estimated_hours']} hours\")\n        print(f\"üòä Sentiment: {classification['sentiment']}\")\n        print(f\"üè∑Ô∏è  Tags: {', '.join(classification['tags'])}\")\n        print(f\"üí¨ Response: {result['generated_response'][:100]}...\")\n        \n        test_results.append(result)\n        \n    except Exception as e:\n        print(f\"‚ùå Error processing ticket: {e}\")\n        test_results.append(None)\n\nprint(f\"\\n‚úÖ Testing complete! Processed {len([r for r in test_results if r])} tickets successfully.\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "govvj69vbk",
   "source": "# Step 3: Initialize LLaMA Customer Support Model\n\nprint(\"üöÄ Initializing LLaMA Customer Support Intelligence Model...\")\n\n# Initialize the model with the appropriate configuration\nif model_path:\n    # Use LLaMA model\n    llama_model = LlamaCustomerSupportModel(\n        model_type=\"llama-cpp\",\n        model_path=model_path\n    )\nelse:\n    # Use fallback model\n    llama_model = LlamaCustomerSupportModel(model_type=\"transformers\")\n\n# Setup the model\nllama_model.setup_model()\n\nprint(\"‚úÖ Model initialization complete!\")\nprint(f\"Model type: {llama_model.model_type}\")\n\n# Display model configuration\nprint(\"\\nüìã Model Configuration:\")\nconfig_dict = {\n    'max_tokens': llama_model.config.max_tokens,\n    'temperature': llama_model.config.temperature,\n    'top_p': llama_model.config.top_p,\n    'top_k': llama_model.config.top_k,\n    'repeat_penalty': llama_model.config.repeat_penalty\n}\n\nfor key, value in config_dict.items():\n    print(f\"- {key}: {value}\")\n\nprint(\"\\nüéØ Supported Categories:\", llama_model.categories)\nprint(\"‚ö° Priority Levels:\", llama_model.priority_levels)\nprint(\"üòä Sentiment Types:\", llama_model.sentiment_types)",
   "metadata": {},
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}