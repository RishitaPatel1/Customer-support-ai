{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü¶ô LLaMA Model Download & Setup\n",
    "\n",
    "This notebook downloads and sets up LLaMA models for the Customer Support Intelligence System.\n",
    "\n",
    "**Run this notebook FIRST** before running other notebooks.\n",
    "\n",
    "## What this notebook does:\n",
    "- ‚úÖ Checks system requirements\n",
    "- ‚úÖ Downloads appropriate LLaMA model\n",
    "- ‚úÖ Tests the model setup\n",
    "- ‚úÖ Prepares the environment for other notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_if_missing(package):\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# Essential packages for this notebook\n",
    "install_if_missing(\"huggingface_hub\")\n",
    "install_if_missing(\"psutil\")\n",
    "install_if_missing(\"requests\")\n",
    "\n",
    "print(\"‚úÖ Required packages ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import psutil\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from huggingface_hub import hf_hub_download\n",
    "import json\n",
    "import time\n",
    "\n",
    "print(\"ü¶ô LLaMA Model Download & Setup\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Windows-specific setup check\nimport platform\n\ndef check_windows_setup():\n    \"\"\"Check if we're on Windows and handle llama-cpp-python issues\"\"\"\n    \n    is_windows = platform.system() == \"Windows\"\n    \n    print(f\"üñ•Ô∏è  Operating System: {platform.system()} {platform.release()}\")\n    \n    if is_windows:\n        print(\"ü™ü Windows detected - checking llama-cpp-python availability...\")\n        \n        try:\n            import llama_cpp\n            print(\"‚úÖ llama-cpp-python is available!\")\n            return True\n        except ImportError:\n            print(\"‚ö†Ô∏è  llama-cpp-python not available (common on Windows)\")\n            print(\"   üìù This is normal - Windows often has build issues\")\n            print(\"   üîÑ Will use HuggingFace transformers instead\")\n            print(\"   ‚ú® You'll still get excellent LLaMA support!\")\n            return False\n    else:\n        print(\"üêß Non-Windows system - llama-cpp-python should work fine\")\n        return True\n\nwindows_llama_available = check_windows_setup()\nprint()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Check System Requirements\n",
    "def check_system_requirements():\n",
    "    \"\"\"Check if system can handle LLaMA model\"\"\"\n",
    "    total_ram = psutil.virtual_memory().total / (1024**3)  # GB\n",
    "    available_ram = psutil.virtual_memory().available / (1024**3)  # GB\n",
    "    disk_free = psutil.disk_usage('.').free / (1024**3)  # GB\n",
    "    \n",
    "    print(f\"üíª System Information:\")\n",
    "    print(f\"   Total RAM: {total_ram:.1f} GB\")\n",
    "    print(f\"   Available RAM: {available_ram:.1f} GB\")\n",
    "    print(f\"   Free Disk Space: {disk_free:.1f} GB\")\n",
    "    print()\n",
    "    \n",
    "    # Determine best model\n",
    "    if available_ram >= 16 and disk_free >= 15:\n",
    "        recommended = \"13B\"\n",
    "        print(\"‚úÖ System suitable for LLaMA 13B model (best quality)\")\n",
    "    elif available_ram >= 8 and disk_free >= 8:\n",
    "        recommended = \"7B\"\n",
    "        print(\"‚úÖ System suitable for LLaMA 7B model (good quality)\")\n",
    "    else:\n",
    "        recommended = \"fallback\"\n",
    "        print(\"‚ö†Ô∏è  Limited resources - will use fallback model\")\n",
    "        if disk_free < 5:\n",
    "            print(\"   ‚ö†Ô∏è  Warning: Low disk space. Need at least 5GB free.\")\n",
    "    \n",
    "    return recommended, available_ram, disk_free\n",
    "\n",
    "recommended_model, ram, disk = check_system_requirements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Model Configuration\n",
    "model_configs = {\n",
    "    \"7B\": {\n",
    "        \"repo_id\": \"TheBloke/Llama-2-7B-Chat-GGUF\",\n",
    "        \"filename\": \"llama-2-7b-chat.Q4_K_M.gguf\",\n",
    "        \"size_gb\": 3.8,\n",
    "        \"description\": \"7B parameter model, 4-bit quantized, good for most systems\"\n",
    "    },\n",
    "    \"13B\": {\n",
    "        \"repo_id\": \"TheBloke/Llama-2-13B-Chat-GGUF\",\n",
    "        \"filename\": \"llama-2-13b-chat.Q6_K.gguf\", \n",
    "        \"size_gb\": 10.3,\n",
    "        \"description\": \"13B parameter model, 6-bit quantized, best quality\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üìã Available Models:\")\n",
    "for size, config in model_configs.items():\n",
    "    print(f\"   {size}: {config['description']} ({config['size_gb']}GB)\")\n",
    "print(f\"\\nüéØ Recommended for your system: {recommended_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Download LLaMA Model\n",
    "def download_llama_model(model_size=\"7B\"):\n",
    "    \"\"\"Download LLaMA model from HuggingFace\"\"\"\n",
    "    \n",
    "    if model_size not in model_configs:\n",
    "        print(f\"‚ùå Invalid model size: {model_size}\")\n",
    "        return None\n",
    "    \n",
    "    config = model_configs[model_size]\n",
    "    \n",
    "    # Create models directory\n",
    "    model_dir = Path(\"../models/llama\")\n",
    "    model_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    model_path = model_dir / config[\"filename\"]\n",
    "    \n",
    "    # Check if already downloaded\n",
    "    if model_path.exists():\n",
    "        print(f\"‚úÖ Model already exists: {model_path}\")\n",
    "        return str(model_path)\n",
    "    \n",
    "    print(f\"‚¨áÔ∏è  Downloading LLaMA {model_size} model...\")\n",
    "    print(f\"   Repository: {config['repo_id']}\")\n",
    "    print(f\"   File: {config['filename']}\")\n",
    "    print(f\"   Size: ~{config['size_gb']}GB\")\n",
    "    print(f\"   This may take 10-30 minutes...\")\n",
    "    print()\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        downloaded_path = hf_hub_download(\n",
    "            repo_id=config[\"repo_id\"],\n",
    "            filename=config[\"filename\"],\n",
    "            local_dir=str(model_dir),\n",
    "            local_dir_use_symlinks=False\n",
    "        )\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"‚úÖ Download completed in {elapsed/60:.1f} minutes!\")\n",
    "        print(f\"   Model saved to: {downloaded_path}\")\n",
    "        \n",
    "        return downloaded_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Download failed: {e}\")\n",
    "        print(\"   Possible solutions:\")\n",
    "        print(\"   1. Check internet connection\")\n",
    "        print(\"   2. Ensure sufficient disk space\")\n",
    "        print(\"   3. Try smaller model (7B instead of 13B)\")\n",
    "        return None\n",
    "\n",
    "# Download the recommended model\n",
    "model_path = None\n",
    "if recommended_model != \"fallback\":\n",
    "    model_path = download_llama_model(recommended_model)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping LLaMA download due to system limitations\")\n",
    "    print(\"   Will use fallback model instead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Test LLaMA Installation\n",
    "if model_path:\n",
    "    try:\n",
    "        # Try to install llama-cpp-python if not available\n",
    "        try:\n",
    "            from llama_cpp import Llama\n",
    "        except ImportError:\n",
    "            print(\"Installing llama-cpp-python...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"llama-cpp-python>=0.2.11\"])\n",
    "            from llama_cpp import Llama\n",
    "        \n",
    "        print(\"üß™ Testing LLaMA model...\")\n",
    "        \n",
    "        # Initialize model\n",
    "        llama = Llama(\n",
    "            model_path=model_path,\n",
    "            n_ctx=512,  # Small context for testing\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Test prompt\n",
    "        test_prompt = \"Hello, I am a customer support AI. How can I help you today?\"\n",
    "        \n",
    "        response = llama(\n",
    "            test_prompt,\n",
    "            max_tokens=50,\n",
    "            temperature=0.1,\n",
    "            echo=False\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ LLaMA model test successful!\")\n",
    "        print(f\"   Test response: {response['choices'][0]['text'].strip()}\")\n",
    "        \n",
    "        # Clean up to free memory\n",
    "        del llama\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  LLaMA test failed: {e}\")\n",
    "        print(\"   The model downloaded but may have issues loading\")\n",
    "        print(\"   Will use fallback model in notebooks\")\n",
    "        model_path = None\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No LLaMA model to test (using fallback)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Save Configuration\n",
    "setup_config = {\n",
    "    \"model_path\": model_path,\n",
    "    \"recommended_model\": recommended_model,\n",
    "    \"system_ram_gb\": ram,\n",
    "    \"system_disk_gb\": disk,\n",
    "    \"setup_complete\": True,\n",
    "    \"setup_timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "}\n",
    "\n",
    "# Save to file for other notebooks to use\n",
    "config_dir = Path(\"../outputs\")\n",
    "config_dir.mkdir(exist_ok=True)\n",
    "\n",
    "with open(config_dir / \"llama_setup_config.json\", \"w\") as f:\n",
    "    json.dump(setup_config, f, indent=2)\n",
    "\n",
    "print(\"üíæ Setup configuration saved!\")\n",
    "print(f\"   Config file: {config_dir}/llama_setup_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Setup Complete!\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "1. **Run the main notebooks** in order:\n",
    "   - `01_data_collection_and_preprocessing.ipynb`\n",
    "   - `02_model_setup_and_configuration.ipynb` \n",
    "   - `03_ticket_classification_system.ipynb`\n",
    "   - And so on...\n",
    "\n",
    "2. **The notebooks will automatically**:\n",
    "   - Load your downloaded LLaMA model\n",
    "   - Use appropriate fallback if needed\n",
    "   - Process customer support tickets\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "If you encounter issues:\n",
    "1. **Out of memory**: Close other applications, restart Jupyter\n",
    "2. **Model errors**: Run this notebook again to re-download\n",
    "3. **Slow performance**: The system will automatically use a smaller model\n",
    "\n",
    "**Ready to process customer support tickets with LLaMA! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}