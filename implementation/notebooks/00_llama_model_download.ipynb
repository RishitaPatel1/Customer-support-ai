{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaMA Model Check\n",
    "\n",
    "This notebook checks if LLaMA has been set up properly.\n",
    "\n",
    "**If you've run LLAMA_SETUP.ipynb successfully, you can skip this notebook entirely.**\n",
    "\n",
    "This notebook will detect your existing LLaMA configuration and confirm it's ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for existing LLaMA setup...\n",
      "‚úÖ LLaMA configuration found!\n",
      "Model: Unknown\n",
      "Architecture: Unknown\n",
      "Optimized for 12GB: False\n",
      "Test Status: FAILED\n",
      "Setup Complete: True\n",
      "\n",
      "‚ö†Ô∏è LLaMA setup exists but test failed\n",
      "Consider re-running LLAMA_SETUP.ipynb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"Checking for existing LLaMA setup...\")\n",
    "\n",
    "# Check if LLAMA_SETUP.ipynb has been run\n",
    "config_path = Path(\"../outputs/llama_setup_config.json\")\n",
    "\n",
    "if config_path.exists():\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    print(\"‚úÖ LLaMA configuration found!\")\n",
    "    print(f\"Model: {config.get('model_name', 'Unknown')}\")\n",
    "    print(f\"Architecture: {config.get('architecture', 'Unknown')}\")\n",
    "    print(f\"Optimized for 12GB: {config.get('optimized_for_12gb', False)}\")\n",
    "    print(f\"Test Status: {'PASSED' if config.get('test_success', False) else 'FAILED'}\")\n",
    "    print(f\"Setup Complete: {config.get('setup_complete', False)}\")\n",
    "    \n",
    "    if config.get('test_success', False):\n",
    "        print(\"\\nüéâ Your LLaMA setup is working perfectly!\")\n",
    "        print(\"You can proceed with the other notebooks (01, 02, 03, etc.)\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è LLaMA setup exists but test failed\")\n",
    "        print(\"Consider re-running LLAMA_SETUP.ipynb\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No LLaMA configuration found\")\n",
    "    print(\"\\nüìã Next steps:\")\n",
    "    print(\"1. Run LLAMA_SETUP.ipynb first\")\n",
    "    print(\"2. That notebook will download and configure LLaMA for your 12GB system\")\n",
    "    print(\"3. Then return to run the other notebooks (01, 02, 03, etc.)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Configuration exists but model name is missing\n",
      "\n",
      "LLaMA check complete!\n"
     ]
    }
   ],
   "source": [
    "# If configuration exists, verify the model files are accessible\n",
    "if config_path.exists():\n",
    "    try:\n",
    "        # Test if we can load the model configuration\n",
    "        model_name = config.get('model_name')\n",
    "        if model_name:\n",
    "            print(f\"\\nVerifying model accessibility: {model_name}\")\n",
    "            \n",
    "            from transformers import AutoTokenizer\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            print(\"‚úÖ Model files are accessible\")\n",
    "            \n",
    "            # Check the configuration details\n",
    "            print(f\"\\nConfiguration details:\")\n",
    "            print(f\"- Force LLaMA: {config.get('force_llama', False)}\")\n",
    "            print(f\"- No Fallbacks: {config.get('no_fallbacks', False)}\")\n",
    "            print(f\"- Memory Optimized: {config.get('memory_optimized', False)}\")\n",
    "            print(f\"- Recommended Mode: {config.get('recommended_mode', 'transformers')}\")\n",
    "            \n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Configuration exists but model name is missing\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Model verification failed: {e}\")\n",
    "        print(\"Consider re-running LLAMA_SETUP.ipynb\")\n",
    "        \n",
    "print(\"\\nLLaMA check complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLaMA Check Complete\n",
    "\n",
    "## Summary:\n",
    "This notebook checks if you have already set up LLaMA using LLAMA_SETUP.ipynb.\n",
    "\n",
    "## Your Setup Status:\n",
    "- If you see \"LLaMA setup is working perfectly!\" above, you're ready to proceed\n",
    "- If you see \"No LLaMA configuration found\", run LLAMA_SETUP.ipynb first\n",
    "\n",
    "## Next Steps:\n",
    "1. **If LLaMA is ready:** Proceed to notebooks 01, 02, 03, 04, 05, 06\n",
    "2. **If LLaMA needs setup:** Run LLAMA_SETUP.ipynb first\n",
    "\n",
    "## Remember:\n",
    "LLAMA_SETUP.ipynb is specifically optimized for your 12GB Intel i7 system and uses TinyLlama (LLaMA architecture) which works perfectly with your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
